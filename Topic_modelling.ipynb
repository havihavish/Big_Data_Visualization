{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic_modelling",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNe/QFkmHmxSlfTUB3Dc2KJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/havihavish/Big_Data_Visualization/blob/master/Topic_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUZrtp6KB50B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1Nm7Sz2CQXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3 = pd.read_csv('Final_Table_3.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEXiyyZeFcXt",
        "colab_type": "code",
        "outputId": "6fd95006-b706-41ae-b8a7-f144789dffd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df3.columns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'article_id', 'paperId', 'doi', 'title', 'venue', 'year',\n",
              "       'intent', 'isInfluential'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0IMaZcNCYwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3_title = df3['title']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgq7N1eyCejt",
        "colab_type": "code",
        "outputId": "b1517f8a-e128-48ab-9a48-be048415a4b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df3_title.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(173453,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBaAYPdALWH9",
        "colab_type": "text"
      },
      "source": [
        "## Extacting noun phrases "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91cJ0qekCrpt",
        "colab_type": "code",
        "outputId": "fa25c942-7725-42e0-de02-09b501a0819d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# sentence = open('dataset.csv', \"r\")\n",
        "# blob = TextBlob(sentence.read())\n",
        "\n",
        "df3['topics'] = df3['title'].apply(lambda x : TextBlob(x).noun_phrases)\n",
        "\n",
        "# noun_phrase = blob.noun_phrases"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr9h-r7MVcie",
        "colab_type": "code",
        "outputId": "adecb992-d919-4dbf-da95-e2c7af2af977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim import corpora, models\n",
        "import gensim\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# create English stop words list\n",
        "en_stop = stopwords.words('english')\n",
        "\n",
        "# Create p_stemmer of class PorterStemmer\n",
        "p_stemmer = PorterStemmer()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-MD8A3FILIr",
        "colab_type": "text"
      },
      "source": [
        "## Adding all five year topics to one list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwrp91vgIPPp",
        "colab_type": "code",
        "outputId": "8aa8796f-e13c-43b7-8762-998418d42c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "doc_set = []\n",
        "for i in range(1971,2017,5):\n",
        "  print()\n",
        "  temp_df = df3.loc[(df3['year'] >=i) & (df3['year']<i+5),'title']\n",
        "  print(i,temp_df.shape)\n",
        "  temp_list = temp_df.tolist()\n",
        "  doc = ''.join(temp_list)\n",
        "  doc_set.append(doc)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1971 (19,)\n",
            "\n",
            "1976 (23,)\n",
            "\n",
            "1981 (34,)\n",
            "\n",
            "1986 (315,)\n",
            "\n",
            "1991 (1477,)\n",
            "\n",
            "1996 (5560,)\n",
            "\n",
            "2001 (16459,)\n",
            "\n",
            "2006 (39528,)\n",
            "\n",
            "2011 (57424,)\n",
            "\n",
            "2016 (51769,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oea7Kjv9ZPjD",
        "colab_type": "code",
        "outputId": "2a82177f-61a4-4a99-cabf-70d483bad55c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(len(doc_set[i]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1187\n",
            "1221\n",
            "2158\n",
            "20660\n",
            "94547\n",
            "369627\n",
            "1092677\n",
            "2690417\n",
            "3998113\n",
            "3759645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8uGUEFWH2A0",
        "colab_type": "text"
      },
      "source": [
        "## Predicting the most popular topic for 5 years by Extracting keywords by using LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FmgTOWeZGb2",
        "colab_type": "code",
        "outputId": "6163a32f-6fd0-41f2-b885-475666a2bd4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "lda_pred = []\n",
        "for i in doc_set:\n",
        "    print(len(i))\n",
        "    \n",
        "    # clean and tokenize document string\n",
        "    raw = i.lower()\n",
        "    tokens = tokenizer.tokenize(raw)\n",
        "\n",
        "\n",
        "    # remove stop words from tokens\n",
        "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "    \n",
        "    # stem tokens\n",
        "    # stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "    texts = []\n",
        "    # add tokens to list\n",
        "    texts.append(stopped_tokens)\n",
        "\n",
        "    # turn our tokenized documents into a id <-> term dictionary\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "        \n",
        "    # convert tokenized documents into a document-term matrix\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    # generate LDA model\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1,id2word = dictionary, passes=20)\n",
        "    lda_pred.append(ldamodel.print_topics())\n",
        "    print(ldamodel.print_topics(num_words = 4))\n",
        "    print()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1187\n",
            "[(0, '0.035*\"data\" + 0.020*\"disc\" + 0.020*\"representations\" + 0.020*\"system\"')]\n",
            "\n",
            "1221\n",
            "[(0, '0.046*\"retrieval\" + 0.036*\"italic\" + 0.026*\"survey\" + 0.026*\"functional\"')]\n",
            "\n",
            "2158\n",
            "[(0, '0.034*\"information\" + 0.034*\"retrieval\" + 0.021*\"document\" + 0.015*\"system\"')]\n",
            "\n",
            "20660\n",
            "[(0, '0.035*\"information\" + 0.035*\"retrieval\" + 0.024*\"document\" + 0.015*\"text\"')]\n",
            "\n",
            "94547\n",
            "[(0, '0.043*\"information\" + 0.041*\"retrieval\" + 0.018*\"text\" + 0.012*\"using\"')]\n",
            "\n",
            "369627\n",
            "[(0, '0.037*\"information\" + 0.027*\"retrieval\" + 0.017*\"text\" + 0.014*\"based\"')]\n",
            "\n",
            "1092677\n",
            "[(0, '0.024*\"information\" + 0.019*\"retrieval\" + 0.019*\"based\" + 0.015*\"web\"')]\n",
            "\n",
            "2690417\n",
            "[(0, '0.021*\"information\" + 0.020*\"based\" + 0.015*\"retrieval\" + 0.013*\"search\"')]\n",
            "\n",
            "3998113\n",
            "[(0, '0.021*\"based\" + 0.017*\"search\" + 0.015*\"information\" + 0.013*\"using\"')]\n",
            "\n",
            "3759645\n",
            "[(0, '0.018*\"based\" + 0.012*\"information\" + 0.011*\"search\" + 0.011*\"using\"')]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS_0L5O2bPQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "abstract_df = pd.read_csv('research_abstracts.csv',encoding = 'ISO-8859-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdFd-iixLcbb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "14981fd0-c370-4252-cff1-b1a2626175fa"
      },
      "source": [
        "abstract_df = abstract_df.rename(columns={'clean_text': 'Abstract'})\n",
        "abstract_df.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document_id</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The concept of maximum entropy can be traced b...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>In most natural language processing applicatio...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Introduction  Statistical natural language pro...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Robots that interact with humans face-to-face ...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Natural languages are languages spoken by huma...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Document_id                                           Abstract Sentiment\n",
              "0            1  The concept of maximum entropy can be traced b...   Neutral\n",
              "1            2  In most natural language processing applicatio...  Positive\n",
              "2            3  Introduction  Statistical natural language pro...  Positive\n",
              "3            4  Robots that interact with humans face-to-face ...  Positive\n",
              "4            5  Natural languages are languages spoken by huma...  Negative"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtgAX8fMMhLZ",
        "colab_type": "text"
      },
      "source": [
        "## Predicting the topic of each abstract by Extracting keywords by using LDA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQEXyQCAMKcl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9acf2b3-d564-473b-feb1-97d763622428"
      },
      "source": [
        "lda_pred = []\n",
        "for i in abstract_df['Abstract'].tolist():\n",
        "       \n",
        "    # clean and tokenize document string\n",
        "    raw = i.lower()\n",
        "    tokens = tokenizer.tokenize(raw)\n",
        "\n",
        "\n",
        "    # remove stop words from tokens\n",
        "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "    \n",
        "    # stem tokens\n",
        "    # stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "    texts = []\n",
        "    # add tokens to list\n",
        "    texts.append(stopped_tokens)\n",
        "\n",
        "    # turn our tokenized documents into a id <-> term dictionary\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "        \n",
        "    # convert tokenized documents into a document-term matrix\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    # generate LDA model\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=1,id2word = dictionary, passes=20)\n",
        "    lda_pred.append(ldamodel.print_topics())\n",
        "    print(ldamodel.print_topics(num_words = 10))\n",
        "    print()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.050*\"maximum\" + 0.040*\"entropy\" + 0.030*\"statistical\" + 0.030*\"approach\" + 0.030*\"problems\" + 0.030*\"describe\" + 0.030*\"concept\" + 0.020*\"processing\" + 0.020*\"powerful\" + 0.020*\"permit\"')]\n",
            "\n",
            "[(0, '0.052*\"semantic\" + 0.052*\"language\" + 0.052*\"natural\" + 0.052*\"description\" + 0.052*\"logics\" + 0.039*\"processing\" + 0.039*\"used\" + 0.039*\"interpretation\" + 0.026*\"processes\" + 0.026*\"needed\"')]\n",
            "\n",
            "[(0, '0.034*\"natural\" + 0.034*\"statistical\" + 0.034*\"language\" + 0.034*\"processing\" + 0.027*\"techniques\" + 0.027*\"speech\" + 0.020*\"learning\" + 0.020*\"machine\" + 0.020*\"snlp\" + 0.020*\"recognition\"')]\n",
            "\n",
            "[(0, '0.066*\"language\" + 0.066*\"robots\" + 0.049*\"natural\" + 0.049*\"humans\" + 0.049*\"face\" + 0.033*\"semantic\" + 0.033*\"situations\" + 0.033*\"spoken\" + 0.033*\"utterances\" + 0.033*\"system\"')]\n",
            "\n",
            "[(0, '0.048*\"natural\" + 0.038*\"nlp\" + 0.038*\"languages\" + 0.038*\"language\" + 0.029*\"paper\" + 0.029*\"processing\" + 0.019*\"whole\" + 0.019*\"point\" + 0.019*\"relate\" + 0.019*\"work\"')]\n",
            "\n",
            "[(0, '0.064*\"language\" + 0.060*\"natural\" + 0.026*\"processing\" + 0.021*\"information\" + 0.021*\"languages\" + 0.017*\"computer\" + 0.017*\"nlp\" + 0.017*\"field\" + 0.017*\"computers\" + 0.013*\"human\"')]\n",
            "\n",
            "[(0, '0.047*\"processing\" + 0.031*\"natural\" + 0.031*\"inference\" + 0.031*\"semantic\" + 0.031*\"discourse\" + 0.031*\"sense\" + 0.031*\"language\" + 0.024*\"syntactic\" + 0.024*\"broad\" + 0.024*\"including\"')]\n",
            "\n",
            "[(0, '0.042*\"analysis\" + 0.042*\"lyrics\" + 0.034*\"music\" + 0.025*\"language\" + 0.025*\"song\" + 0.025*\"results\" + 0.025*\"acoustic\" + 0.017*\"categorization\" + 0.017*\"searches\" + 0.017*\"significant\"')]\n",
            "\n",
            "[(0, '0.038*\"retrieval\" + 0.026*\"paper\" + 0.026*\"document\" + 0.019*\"language\" + 0.019*\"properties\" + 0.019*\"processing\" + 0.019*\"data\" + 0.019*\"natural\" + 0.019*\"text\" + 0.013*\"new\"')]\n",
            "\n",
            "[(0, '0.052*\"processing\" + 0.039*\"several\" + 0.039*\"models\" + 0.039*\"natural\" + 0.039*\"connectionist\" + 0.039*\"language\" + 0.026*\"parallel\" + 0.026*\"promising\" + 0.026*\"localist\" + 0.026*\"paper\"')]\n",
            "\n",
            "[(0, '0.048*\"learning\" + 0.048*\"approach\" + 0.032*\"number\" + 0.032*\"paper\" + 0.032*\"part\" + 0.032*\"performance\" + 0.032*\"present\" + 0.032*\"method\" + 0.032*\"rule\" + 0.032*\"tasks\"')]\n",
            "\n",
            "[(0, '0.040*\"language\" + 0.040*\"feature\" + 0.040*\"word\" + 0.027*\"new\" + 0.027*\"letters\" + 0.027*\"map\" + 0.027*\"meaning\" + 0.027*\"modeled\" + 0.027*\"natural\" + 0.027*\"process\"')]\n",
            "\n",
            "[(0, '0.022*\"tagging\" + 0.022*\"task\" + 0.022*\"basis\" + 0.022*\"system\" + 0.015*\"propose\" + 0.015*\"prior\" + 0.015*\"requirements\" + 0.015*\"representations\" + 0.015*\"recognition\" + 0.015*\"semantic\"')]\n",
            "\n",
            "[(0, '0.041*\"86\" + 0.031*\"core\" + 0.031*\"point\" + 0.031*\"issues\" + 0.021*\"principles\" + 0.021*\"paper\" + 0.021*\"process\" + 0.021*\"nlp\" + 0.021*\"naturally\" + 0.021*\"owens\"')]\n",
            "\n",
            "[(0, '0.066*\"nlp\" + 0.026*\"medical\" + 0.020*\"common\" + 0.020*\"problems\" + 0.020*\"field\" + 0.020*\"possible\" + 0.020*\"sub\" + 0.020*\"provide\" + 0.020*\"limited\" + 0.020*\"tutorial\"')]\n",
            "\n",
            "[(0, '0.033*\"used\" + 0.027*\"grain\" + 0.022*\"captions\" + 0.022*\"form\" + 0.022*\"logical\" + 0.022*\"knowledge\" + 0.022*\"match\" + 0.016*\"query\" + 0.016*\"semantic\" + 0.016*\"keywords\"')]\n",
            "\n",
            "[(0, '0.036*\"transduction\" + 0.036*\"signal\" + 0.029*\"system\" + 0.029*\"agent\" + 0.022*\"information\" + 0.022*\"machinery\" + 0.022*\"pathways\" + 0.022*\"results\" + 0.022*\"knowledge\" + 0.014*\"multi\"')]\n",
            "\n",
            "[(0, '0.050*\"content\" + 0.029*\"web\" + 0.021*\"noisy\" + 0.021*\"pages\" + 0.021*\"analysis\" + 0.017*\"information\" + 0.017*\"main\" + 0.012*\"extraction\" + 0.012*\"templates\" + 0.012*\"software\"')]\n",
            "\n",
            "[(0, '0.068*\"evaluation\" + 0.034*\"nlp\" + 0.027*\"system\" + 0.027*\"part\" + 0.020*\"strategies\" + 0.020*\"particular\" + 0.020*\"analysis\" + 0.014*\"related\" + 0.014*\"proper\" + 0.014*\"refers\"')]\n",
            "\n",
            "[(0, '0.053*\"language\" + 0.046*\"processing\" + 0.039*\"natural\" + 0.026*\"nltk\" + 0.020*\"discuss\" + 0.020*\"python\" + 0.020*\"range\" + 0.020*\"libraries\" + 0.020*\"suite\" + 0.013*\"tools\"')]\n",
            "\n",
            "[(0, '0.048*\"requirements\" + 0.038*\"nlp\" + 0.029*\"specification\" + 0.029*\"natural\" + 0.029*\"ontologies\" + 0.029*\"language\" + 0.029*\"processing\" + 0.029*\"new\" + 0.029*\"application\" + 0.029*\"use\"')]\n",
            "\n",
            "[(0, '0.042*\"text\" + 0.028*\"processing\" + 0.028*\"serial\" + 0.028*\"nlp\" + 0.028*\"paper\" + 0.028*\"parallel\" + 0.028*\"possible\" + 0.028*\"processes\" + 0.028*\"natural\" + 0.028*\"reviews\"')]\n",
            "\n",
            "[(0, '0.033*\"dictionary\" + 0.033*\"natural\" + 0.033*\"language\" + 0.033*\"grammar\" + 0.033*\"system\" + 0.025*\"linking\" + 0.025*\"lexical\" + 0.025*\"processing\" + 0.025*\"environment\" + 0.025*\"version\"')]\n",
            "\n",
            "[(0, '0.056*\"complexity\" + 0.037*\"natural\" + 0.037*\"language\" + 0.037*\"tasks\" + 0.028*\"processing\" + 0.028*\"nlp\" + 0.028*\"automata\" + 0.019*\"predicting\" + 0.019*\"positions\" + 0.019*\"various\"')]\n",
            "\n",
            "[(0, '0.062*\"learning\" + 0.052*\"research\" + 0.041*\"deep\" + 0.031*\"recent\" + 0.031*\"processing\" + 0.031*\"language\" + 0.031*\"natural\" + 0.021*\"motion\" + 0.021*\"process\" + 0.021*\"new\"')]\n",
            "\n",
            "[(0, '0.038*\"retrieval\" + 0.031*\"processing\" + 0.025*\"statistical\" + 0.025*\"methods\" + 0.019*\"documents\" + 0.019*\"good\" + 0.019*\"linguistic\" + 0.013*\"request\" + 0.013*\"related\" + 0.013*\"question\"')]\n",
            "\n",
            "[(0, '0.053*\"voice\" + 0.031*\"features\" + 0.031*\"system\" + 0.023*\"tamil\" + 0.023*\"recognition\" + 0.023*\"samples\" + 0.023*\"feature\" + 0.023*\"digital\" + 0.023*\"extract\" + 0.023*\"match\"')]\n",
            "\n",
            "[(0, '0.070*\"requirements\" + 0.056*\"testing\" + 0.056*\"test\" + 0.042*\"application\" + 0.042*\"approach\" + 0.028*\"testers\" + 0.028*\"paper\" + 0.028*\"performed\" + 0.028*\"presented\" + 0.028*\"often\"')]\n",
            "\n",
            "[(0, '0.038*\"semantic\" + 0.038*\"logics\" + 0.038*\"description\" + 0.031*\"interpretation\" + 0.023*\"process\" + 0.023*\"natural\" + 0.023*\"one\" + 0.023*\"processing\" + 0.023*\"language\" + 0.023*\"syntactic\"')]\n",
            "\n",
            "[(0, '0.046*\"humor\" + 0.023*\"understanding\" + 0.023*\"face\" + 0.017*\"information\" + 0.017*\"act\" + 0.017*\"general\" + 0.017*\"workshop\" + 0.017*\"us\" + 0.017*\"interaction\" + 0.017*\"modelling\"')]\n",
            "\n",
            "[(0, '0.030*\"keyfact\" + 0.024*\"query\" + 0.024*\"documents\" + 0.018*\"rate\" + 0.018*\"term\" + 0.018*\"semantic\" + 0.018*\"relevant\" + 0.018*\"knowledge\" + 0.018*\"precision\" + 0.018*\"information\"')]\n",
            "\n",
            "[(0, '0.038*\"patterns\" + 0.025*\"musical\" + 0.019*\"music\" + 0.019*\"solo\" + 0.019*\"progression\" + 0.019*\"structure\" + 0.019*\"composition\" + 0.019*\"propagation\" + 0.019*\"pattern\" + 0.019*\"whole\"')]\n",
            "\n",
            "[(0, '0.053*\"thesauruses\" + 0.053*\"nlp\" + 0.039*\"evaluation\" + 0.039*\"manual\" + 0.039*\"tasks\" + 0.039*\"thesaurus\" + 0.039*\"range\" + 0.039*\"within\" + 0.026*\"roles\" + 0.026*\"resources\"')]\n",
            "\n",
            "[(0, '0.051*\"language\" + 0.051*\"systems\" + 0.051*\"natural\" + 0.026*\"qualifying\" + 0.026*\"logic\" + 0.026*\"modules\" + 0.026*\"paper\" + 0.026*\"processing\" + 0.026*\"representation\" + 0.026*\"reasoning\"')]\n",
            "\n",
            "[(0, '0.031*\"dgn\" + 0.031*\"approach\" + 0.031*\"parsing\" + 0.031*\"word\" + 0.031*\"recognition\" + 0.031*\"network\" + 0.031*\"lattice\" + 0.021*\"principal\" + 0.021*\"results\" + 0.021*\"processing\"')]\n",
            "\n",
            "[(0, '0.014*\"small\" + 0.014*\"million\" + 0.014*\"article\" + 0.014*\"information\" + 0.011*\"new\" + 0.011*\"natural\" + 0.011*\"processing\" + 0.011*\"language\" + 0.011*\"services\" + 0.011*\"large\"')]\n",
            "\n",
            "[(0, '0.038*\"graph\" + 0.038*\"processing\" + 0.038*\"based\" + 0.038*\"text\" + 0.038*\"language\" + 0.025*\"representations\" + 0.025*\"sense\" + 0.025*\"sentiment\" + 0.025*\"subjectivity\" + 0.025*\"successful\"')]\n",
            "\n",
            "[(0, '0.037*\"visual\" + 0.031*\"modules\" + 0.031*\"results\" + 0.025*\"graph\" + 0.025*\"data\" + 0.025*\"development\" + 0.025*\"processing\" + 0.025*\"language\" + 0.019*\"system\" + 0.019*\"Ã¢\"')]\n",
            "\n",
            "[(0, '0.041*\"model\" + 0.031*\"structure\" + 0.031*\"language\" + 0.031*\"learning\" + 0.031*\"machine\" + 0.031*\"tasks\" + 0.031*\"nlp\" + 0.020*\"present\" + 0.020*\"processing\" + 0.020*\"pos\"')]\n",
            "\n",
            "[(0, '0.028*\"research\" + 0.021*\"chapter\" + 0.021*\"nlp\" + 0.021*\"1980s\" + 0.021*\"paradigms\" + 0.021*\"field\" + 0.021*\"two\" + 0.014*\"merging\" + 0.014*\"processing\" + 0.014*\"many\"')]\n",
            "\n",
            "[(0, '0.038*\"nlp\" + 0.026*\"processing\" + 0.019*\"ir\" + 0.019*\"improvements\" + 0.019*\"information\" + 0.019*\"etc\" + 0.019*\"methods\" + 0.019*\"retrieval\" + 0.019*\"level\" + 0.019*\"techniques\"')]\n",
            "\n",
            "[(0, '0.054*\"units\" + 0.034*\"elements\" + 0.030*\"basic\" + 0.025*\"language\" + 0.025*\"natural\" + 0.025*\"distributional\" + 0.025*\"statistical\" + 0.020*\"two\" + 0.020*\"need\" + 0.020*\"static\"')]\n",
            "\n",
            "[(0, '0.037*\"sorting\" + 0.037*\"kernelized\" + 0.037*\"matching\" + 0.028*\"two\" + 0.028*\"objects\" + 0.028*\"tasks\" + 0.028*\"processing\" + 0.028*\"sources\" + 0.018*\"prior\" + 0.018*\"require\"')]\n",
            "\n",
            "[(0, '0.041*\"plan\" + 0.041*\"recognition\" + 0.031*\"results\" + 0.031*\"research\" + 0.031*\"language\" + 0.031*\"parsing\" + 0.031*\"pr\" + 0.031*\"recent\" + 0.031*\"natural\" + 0.031*\"argue\"')]\n",
            "\n",
            "[(0, '0.025*\"models\" + 0.018*\"statistics\" + 0.018*\"model\" + 0.018*\"performance\" + 0.018*\"framework\" + 0.018*\"features\" + 0.018*\"describe\" + 0.018*\"data\" + 0.018*\"important\" + 0.018*\"also\"')]\n",
            "\n",
            "[(0, '0.083*\"logic\" + 0.069*\"programming\" + 0.042*\"knowledge\" + 0.042*\"learning\" + 0.042*\"machine\" + 0.042*\"linguistic\" + 0.042*\"within\" + 0.028*\"building\" + 0.028*\"point\" + 0.028*\"representing\"')]\n",
            "\n",
            "[(0, '0.051*\"tuc\" + 0.041*\"information\" + 0.041*\"natural\" + 0.041*\"language\" + 0.041*\"abstract\" + 0.031*\"knowledge\" + 0.031*\"supplied\" + 0.031*\"retrieval\" + 0.031*\"needed\" + 0.031*\"documents\"')]\n",
            "\n",
            "[(0, '0.062*\"methods\" + 0.047*\"statistical\" + 0.023*\"nlp\" + 0.023*\"three\" + 0.023*\"show\" + 0.023*\"language\" + 0.023*\"kinds\" + 0.023*\"natural\" + 0.016*\"notion\" + 0.016*\"probability\"')]\n",
            "\n",
            "[(0, '0.051*\"nlp\" + 0.025*\"subtopics\" + 0.025*\"problems\" + 0.025*\"research\" + 0.019*\"language\" + 0.019*\"provide\" + 0.019*\"recognition\" + 0.019*\"speech\" + 0.019*\"done\" + 0.019*\"sub\"')]\n",
            "\n",
            "[(0, '0.042*\"driven\" + 0.035*\"syntax\" + 0.028*\"based\" + 0.028*\"semantics\" + 0.028*\"interpretation\" + 0.028*\"approaches\" + 0.028*\"parsers\" + 0.028*\"use\" + 0.021*\"process\" + 0.021*\"parse\"')]\n",
            "\n",
            "[(0, '0.073*\"language\" + 0.049*\"history\" + 0.049*\"work\" + 0.049*\"thirtyfive\" + 0.049*\"processing\" + 0.049*\"opportunities\" + 0.049*\"natural\" + 0.049*\"learning\" + 0.049*\"last\" + 0.049*\"application\"')]\n",
            "\n",
            "[(0, '0.027*\"learning\" + 0.027*\"de\" + 0.027*\"la\" + 0.021*\"natural\" + 0.016*\"language\" + 0.016*\"part\" + 0.016*\"document\" + 0.016*\"soria\" + 0.016*\"wsd\" + 0.016*\"machine\"')]\n",
            "\n",
            "[(0, '0.050*\"evaluation\" + 0.030*\"special\" + 0.030*\"processing\" + 0.030*\"language\" + 0.030*\"issue\" + 0.030*\"natural\" + 0.020*\"prospective\" + 0.020*\"underlying\" + 0.020*\"protocol\" + 0.020*\"remarks\"')]\n",
            "\n",
            "[(0, '0.031*\"learning\" + 0.025*\"extraction\" + 0.025*\"machine\" + 0.025*\"new\" + 0.019*\"mainly\" + 0.019*\"data\" + 0.019*\"linguistic\" + 0.019*\"task\" + 0.019*\"information\" + 0.019*\"algorithm\"')]\n",
            "\n",
            "[(0, '0.041*\"probabilistic\" + 0.031*\"language\" + 0.031*\"fsts\" + 0.031*\"modeling\" + 0.031*\"tree\" + 0.031*\"methods\" + 0.031*\"string\" + 0.031*\"transducers\" + 0.031*\"work\" + 0.020*\"processing\"')]\n",
            "\n",
            "[(0, '0.040*\"nlp\" + 0.030*\"applications\" + 0.030*\"domains\" + 0.030*\"reports\" + 0.030*\"multiple\" + 0.030*\"system\" + 0.020*\"paper\" + 0.020*\"originally\" + 0.020*\"particular\" + 0.020*\"processing\"')]\n",
            "\n",
            "[(0, '0.026*\"prolog\" + 0.026*\"ie\" + 0.022*\"application\" + 0.019*\"kb\" + 0.019*\"annotated\" + 0.019*\"facts\" + 0.015*\"information\" + 0.015*\"system\" + 0.015*\"documents\" + 0.015*\"base\"')]\n",
            "\n",
            "[(0, '0.063*\"retrieval\" + 0.048*\"system\" + 0.032*\"search\" + 0.032*\"queries\" + 0.032*\"ranking\" + 0.032*\"response\" + 0.032*\"processing\" + 0.032*\"statistical\" + 0.032*\"uses\" + 0.032*\"traditional\"')]\n",
            "\n",
            "[(0, '0.051*\"systems\" + 0.042*\"context\" + 0.042*\"processing\" + 0.034*\"language\" + 0.034*\"natural\" + 0.025*\"variant\" + 0.025*\"open\" + 0.025*\"domains\" + 0.025*\"issues\" + 0.017*\"paper\"')]\n",
            "\n",
            "[(0, '0.050*\"language\" + 0.050*\"feasible\" + 0.050*\"text\" + 0.050*\"techniques\" + 0.050*\"students\" + 0.050*\"processing\" + 0.050*\"practical\" + 0.050*\"new\" + 0.050*\"natural\" + 0.050*\"2004\"')]\n",
            "\n",
            "[(0, '0.043*\"actions\" + 0.043*\"system\" + 0.029*\"set\" + 0.029*\"new\" + 0.029*\"process\" + 0.029*\"schema\" + 0.021*\"causal\" + 0.021*\"achieve\" + 0.021*\"goal\" + 0.021*\"narrative\"')]\n",
            "\n",
            "[(0, '0.042*\"language\" + 0.036*\"processing\" + 0.036*\"natural\" + 0.030*\"india\" + 0.018*\"information\" + 0.018*\"research\" + 0.018*\"field\" + 0.018*\"paper\" + 0.018*\"indian\" + 0.018*\"speech\"')]\n",
            "\n",
            "[(0, '0.052*\"lolita\" + 0.052*\"systems\" + 0.039*\"evaluation\" + 0.039*\"processing\" + 0.039*\"nlp\" + 0.039*\"natural\" + 0.039*\"language\" + 0.039*\"university\" + 0.039*\"durham\" + 0.026*\"questions\"')]\n",
            "\n",
            "[(0, '0.046*\"web\" + 0.036*\"models\" + 0.031*\"counts\" + 0.026*\"tasks\" + 0.026*\"based\" + 0.015*\"unsupervised\" + 0.015*\"supervised\" + 0.015*\"corpus\" + 0.015*\"nlp\" + 0.015*\"rather\"')]\n",
            "\n",
            "[(0, '0.033*\"language\" + 0.033*\"chapter\" + 0.026*\"call\" + 0.026*\"programs\" + 0.026*\"natural\" + 0.026*\"cl\" + 0.020*\"focus\" + 0.020*\"help\" + 0.020*\"languages\" + 0.020*\"linguistics\"')]\n",
            "\n",
            "[(0, '0.056*\"machine\" + 0.056*\"kernels\" + 0.056*\"state\" + 0.056*\"recognised\" + 0.056*\"processes\" + 0.056*\"powerful\" + 0.056*\"mod\" + 0.056*\"many\" + 0.056*\"art\" + 0.056*\"learning\"')]\n",
            "\n",
            "[(0, '0.071*\"confidence\" + 0.057*\"language\" + 0.043*\"processing\" + 0.043*\"natural\" + 0.043*\"estimation\" + 0.043*\"machine\" + 0.043*\"measures\" + 0.029*\"recognition\" + 0.029*\"results\" + 0.029*\"present\"')]\n",
            "\n",
            "[(0, '0.048*\"sense\" + 0.042*\"information\" + 0.042*\"id\" + 0.030*\"sign\" + 0.030*\"lkb\" + 0.024*\"lex\" + 0.018*\"9\" + 0.018*\"representation\" + 0.018*\"type\" + 0.018*\"ldoce\"')]\n",
            "\n",
            "[(0, '0.045*\"language\" + 0.034*\"natural\" + 0.034*\"acquisition\" + 0.034*\"domain\" + 0.034*\"processing\" + 0.034*\"knowledge\" + 0.022*\"paper\" + 0.022*\"prerequisite\" + 0.022*\"process\" + 0.022*\"preprogrammed\"')]\n",
            "\n",
            "[(0, '0.036*\"word\" + 0.024*\"som\" + 0.018*\"language\" + 0.018*\"maps\" + 0.018*\"self\" + 0.018*\"information\" + 0.018*\"classes\" + 0.018*\"organizing\" + 0.018*\"map\" + 0.018*\"nodes\"')]\n",
            "\n",
            "[(0, '0.052*\"two\" + 0.052*\"diff\" + 0.041*\"different\" + 0.031*\"natural\" + 0.031*\"differences\" + 0.031*\"language\" + 0.031*\"data\" + 0.031*\"datasets\" + 0.031*\"sets\" + 0.031*\"processing\"')]\n",
            "\n",
            "[(0, '0.037*\"learning\" + 0.037*\"tasks\" + 0.029*\"language\" + 0.022*\"multitask\" + 0.022*\"supervised\" + 0.022*\"sentence\" + 0.022*\"semi\" + 0.022*\"model\" + 0.022*\"using\" + 0.022*\"tags\"')]\n",
            "\n",
            "[(0, '0.054*\"statistics\" + 0.054*\"text\" + 0.036*\"stylometry\" + 0.036*\"processing\" + 0.036*\"program\" + 0.036*\"prolog\" + 0.036*\"paper\" + 0.036*\"studies\" + 0.036*\"used\" + 0.036*\"usage\"')]\n",
            "\n",
            "[(0, '0.051*\"tools\" + 0.051*\"processing\" + 0.051*\"workbench\" + 0.034*\"resources\" + 0.034*\"set\" + 0.034*\"proofing\" + 0.034*\"purposes\" + 0.034*\"presents\" + 0.034*\"text\" + 0.034*\"technology\"')]\n",
            "\n",
            "[(0, '0.046*\"language\" + 0.026*\"symbolic\" + 0.026*\"processing\" + 0.026*\"linguistic\" + 0.020*\"grammar\" + 0.020*\"natural\" + 0.020*\"structures\" + 0.020*\"knowledge\" + 0.020*\"world\" + 0.013*\"specified\"')]\n",
            "\n",
            "[(0, '0.037*\"model\" + 0.037*\"upper\" + 0.037*\"domain\" + 0.022*\"resource\" + 0.022*\"knowledge\" + 0.022*\"general\" + 0.015*\"reusable\" + 0.015*\"results\" + 0.015*\"resources\" + 0.015*\"significantly\"')]\n",
            "\n",
            "[(0, '0.042*\"lal\" + 0.034*\"annotation\" + 0.034*\"programs\" + 0.034*\"processing\" + 0.034*\"language\" + 0.025*\"natural\" + 0.025*\"developed\" + 0.025*\"linguistic\" + 0.025*\"nlp\" + 0.025*\"translation\"')]\n",
            "\n",
            "[(0, '0.052*\"language\" + 0.035*\"processing\" + 0.035*\"network\" + 0.035*\"neural\" + 0.035*\"natural\" + 0.026*\"processes\" + 0.026*\"computing\" + 0.026*\"technology\" + 0.017*\"patterns\" + 0.017*\"observe\"')]\n",
            "\n",
            "[(0, '0.051*\"data\" + 0.034*\"size\" + 0.034*\"raw\" + 0.034*\"recent\" + 0.034*\"research\" + 0.034*\"search\" + 0.034*\"presence\" + 0.034*\"standard\" + 0.034*\"world\" + 0.034*\"web\"')]\n",
            "\n",
            "[(0, '0.062*\"language\" + 0.038*\"grammatical\" + 0.038*\"complete\" + 0.038*\"structure\" + 0.038*\"word\" + 0.038*\"linguistic\" + 0.038*\"system\" + 0.038*\"detailed\" + 0.038*\"us\" + 0.038*\"understand\"')]\n",
            "\n",
            "[(0, '0.067*\"illustrate\" + 0.067*\"hard\" + 0.067*\"sufficiently\" + 0.067*\"simply\" + 0.067*\"seenled\" + 0.067*\"primitives\" + 0.067*\"links\" + 0.067*\"anything\" + 0.067*\"definition\" + 0.067*\"compiled\"')]\n",
            "\n",
            "[(0, '0.030*\"language\" + 0.030*\"educational\" + 0.026*\"natural\" + 0.026*\"learning\" + 0.017*\"processing\" + 0.017*\"process\" + 0.017*\"nlp\" + 0.017*\"context\" + 0.017*\"effective\" + 0.017*\"education\"')]\n",
            "\n",
            "[(0, '0.073*\"different\" + 0.073*\"framenet\" + 0.073*\"nlp\" + 0.049*\"projects\" + 0.049*\"overcome\" + 0.049*\"using\" + 0.049*\"two\" + 0.049*\"summarize\" + 0.049*\"sketch\" + 0.049*\"rather\"')]\n",
            "\n",
            "[(0, '0.043*\"stage\" + 0.032*\"semi\" + 0.032*\"approach\" + 0.032*\"automated\" + 0.032*\"analysis\" + 0.021*\"requirement\" + 0.021*\"requirements\" + 0.021*\"performs\" + 0.021*\"presented\" + 0.021*\"provides\"')]\n",
            "\n",
            "[(0, '0.064*\"language\" + 0.045*\"nodes\" + 0.036*\"subsumption\" + 0.027*\"network\" + 0.027*\"representation\" + 0.027*\"semantic\" + 0.027*\"natural\" + 0.027*\"structure\" + 0.027*\"description\" + 0.027*\"complex\"')]\n",
            "\n",
            "[(0, '0.041*\"modelling\" + 0.041*\"text\" + 0.041*\"language\" + 0.041*\"character\" + 0.027*\"problems\" + 0.027*\"show\" + 0.027*\"perform\" + 0.027*\"powerful\" + 0.027*\"predicting\" + 0.027*\"number\"')]\n",
            "\n",
            "[(0, '0.038*\"report\" + 0.038*\"performance\" + 0.038*\"impact\" + 0.038*\"subproject\" + 0.038*\"context\" + 0.038*\"hpcn\" + 0.038*\"high\" + 0.038*\"project\" + 0.026*\"resources\" + 0.026*\"organization\"')]\n",
            "\n",
            "[(0, '0.066*\"xml\" + 0.066*\"queries\" + 0.053*\"ir\" + 0.053*\"language\" + 0.039*\"nlqs\" + 0.039*\"formal\" + 0.026*\"retrieval\" + 0.026*\"specific\" + 0.026*\"respond\" + 0.026*\"systems\"')]\n",
            "\n",
            "[(0, '0.030*\"use\" + 0.030*\"toolkit\" + 0.030*\"design\" + 0.030*\"analysis\" + 0.030*\"nlp\" + 0.020*\"provides\" + 0.020*\"requiring\" + 0.020*\"quite\" + 0.020*\"open\" + 0.020*\"quality\"')]\n",
            "\n",
            "[(0, '0.044*\"documents\" + 0.037*\"english\" + 0.030*\"translated\" + 0.030*\"title\" + 0.022*\"statistics\" + 0.022*\"generated\" + 0.022*\"generation\" + 0.022*\"several\" + 0.022*\"original\" + 0.022*\"machine\"')]\n",
            "\n",
            "[(0, '0.077*\"reasoning\" + 0.051*\"natural\" + 0.051*\"mechanism\" + 0.051*\"system\" + 0.051*\"reports\" + 0.051*\"realize\" + 0.051*\"production\" + 0.051*\"preliminary\" + 0.051*\"paper\" + 0.051*\"carry\"')]\n",
            "\n",
            "[(0, '0.034*\"system\" + 0.034*\"processing\" + 0.034*\"knowledge\" + 0.027*\"kb\" + 0.027*\"information\" + 0.020*\"extraction\" + 0.020*\"way\" + 0.020*\"patient\" + 0.020*\"hobbs\" + 0.020*\"1993\"')]\n",
            "\n",
            "[(0, '0.044*\"systems\" + 0.044*\"reporting\" + 0.035*\"crime\" + 0.035*\"information\" + 0.026*\"witnesses\" + 0.026*\"7\" + 0.026*\"24\" + 0.026*\"based\" + 0.018*\"publicized\" + 0.018*\"ranging\"')]\n",
            "\n",
            "[(0, '0.035*\"curation\" + 0.035*\"integrated\" + 0.035*\"curators\" + 0.024*\"present\" + 0.024*\"processing\" + 0.024*\"real\" + 0.024*\"presents\" + 0.024*\"potential\" + 0.024*\"rigorously\" + 0.024*\"paper\"')]\n",
            "\n",
            "[(0, '0.064*\"language\" + 0.045*\"nodes\" + 0.036*\"subsumption\" + 0.027*\"network\" + 0.027*\"representation\" + 0.027*\"semantic\" + 0.027*\"natural\" + 0.027*\"structure\" + 0.027*\"description\" + 0.027*\"complex\"')]\n",
            "\n",
            "[(0, '0.125*\"quot\" + 0.042*\"lexical\" + 0.042*\"phrases\" + 0.042*\"organization\" + 0.042*\"senses\" + 0.042*\"paper\" + 0.028*\"programs\" + 0.028*\"proposed\" + 0.028*\"process\" + 0.028*\"recognition\"')]\n",
            "\n",
            "[(0, '0.031*\"entities\" + 0.031*\"gives\" + 0.031*\"named\" + 0.031*\"search\" + 0.031*\"user\" + 0.021*\"system\" + 0.021*\"space\" + 0.021*\"systems\" + 0.021*\"result\" + 0.021*\"results\"')]\n",
            "\n",
            "[(0, '0.030*\"information\" + 0.030*\"based\" + 0.030*\"using\" + 0.030*\"kernel\" + 0.020*\"problem\" + 0.020*\"provide\" + 0.020*\"propose\" + 0.020*\"pp\" + 0.020*\"nlp\" + 0.020*\"potentially\"')]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVhEPX9BM-I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}